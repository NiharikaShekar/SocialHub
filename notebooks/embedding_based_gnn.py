# -*- coding: utf-8 -*-
"""Embedding_Based_GNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_jrZp5ygwTdrgnTKGcK7MIwNQ8-xHsx

# Student Profile Analysis and Recommendation

This notebook demonstrates a process for analyzing student profiles and generating friend recommendations based on their characteristics and interests.

## 1. Data Loading and Initial Inspection

This section loads the student profile data from a CSV file and performs an initial inspection of the data, including viewing the first few rows and checking data types and missing values.
"""

# Import the pandas library, the cornerstone for data analysis in Python
import pandas as pd

# Load your processed CSV file into a DataFrame.
# Make sure to replace 'your_processed_profiles.csv' with the actual name of your file.
df = pd.read_csv('profiles_processed.csv')

# Display the first 5 rows to get a quick look at your data
print("First 5 rows of the dataset:")
print(df.head())

# Get a concise summary of the DataFrame, including data types and non-null values.
# This is crucial for identifying which columns might have missing data.
print("\nDataset Information:")
df.info()

"""## 2. Feature Engineering: Text Concatenation

Here, we combine relevant text-based columns (like Hobbies, Unique Quality, and Story) into a single 'profile\_text' column. This consolidated text will be used to generate text embeddings.
"""

# Define the columns that contain narrative text
text_columns = ['Hobbies', 'Unique Quality', 'Story']

# Fill any potential missing values in these columns with an empty string
# This prevents errors and ensures consistency.
for col in text_columns:
    df[col] = df[col].fillna('')

# Concatenate the text columns into a new 'profile_text' column.
# We join the text from each specified column with a space in between.
df['profile_text'] = df[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)

# Display the new column for the first few students to verify
print("\nConsolidated Profile Text for the first 2 students:")
print(df[['Name', 'profile_text']].head(2))

"""## 3. Install Sentence Transformers, Torch etc.

This cell installs the necessary library for generating sentence embeddings.
"""

!pip install -q torch-geometric sentence-transformers

"""## 4. Generate Text Embeddings

This section uses a pre-trained sentence transformer model to convert the 'profile\_text' into numerical vectors (embeddings). These embeddings capture the semantic meaning of the text.
"""

# Import the SentenceTransformer class and the numpy library for numerical operations
from sentence_transformers import SentenceTransformer
import numpy as np

# 1. Load the pre-trained model
# 'all-MiniLM-L6-v2' is a fantastic general-purpose model. It's fast and highly effective
# for semantic similarity tasks, making it ideal for our project.
# The library will download the model automatically on its first run.
print("Loading the Sentence Transformer model...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("Model loaded successfully.")

# 2. Encode the profile_text column
# We convert the 'profile_text' column to a list of strings to feed into the model.
# The model.encode() function will then process each string and output a vector.
# show_progress_bar=True is helpful to monitor progress on larger datasets.
print("\nGenerating text embeddings for all profiles... (This may take a moment)")
text_embeddings = model.encode(df['profile_text'].tolist(), show_progress_bar=True)

# 3. Inspect the result
# The output is a NumPy array where each row is the numerical vector (embedding)
# for a corresponding student in your DataFrame.
print(f"\nEmbeddings generated. The shape of our embeddings matrix is: {text_embeddings.shape}")

# 4. Save the embeddings
# This is a good practice. Generating embeddings can take a few minutes.
# By saving them to a file, you can easily load them back in later
# without needing to re-run the encoding process.
np.save('text_embeddings.npy', text_embeddings)
print("Text embeddings have been saved to 'text_embeddings.npy'")

"""## 5. Feature Engineering: Combine and Normalize Features

We combine the numerical features (Age, GPA), one-hot encoded categorical features (Sex, Major, Year, Country, State/Province), and the text embeddings into a single, unified vector for each student. These vectors are then normalized.
"""

# --- Step 1: Import Necessary Libraries ---
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, normalize


# --- Step 2: Define Numerical and Categorical Features ---
# Based on your data, we'll specify which columns to treat as categorical
# and which to treat as numerical.
categorical_features =['Sex','Major','Year','Country','State/Province']
numerical_features = ['Age', 'GPA']

# --- Step 3: Process Categorical Features with One-Hot Encoding ---
# This technique converts each category into a new binary column (0 or 1).
# It's the standard way to prepare categorical data for machine learning
# because it doesn't create a false sense of order between categories.
print("Processing categorical features with one-hot encoding...")
categorical_df = pd.get_dummies(df[categorical_features], prefix_sep='_', drop_first=True)
print(f"Shape of the one-hot encoded data: {categorical_df.shape}")

# --- Step 4: Process Numerical Features with Standardization ---
# This rescales the numerical data so that each feature has a mean of 0
# and a standard deviation of 1. This is crucial to prevent features with
# larger values (like Age) from disproportionately influencing the model.
print("\nProcessing numerical features with standardization...")
scaler = StandardScaler()
numerical_scaled = scaler.fit_transform(df[numerical_features])
print(f"Shape of the standardized numerical data: {numerical_scaled.shape}")

# --- Step 5: Load Your Pre-Generated Text Embeddings ---
# Load the text_embeddings.npy file you created in the previous step.
print("\nLoading the saved text embeddings...")
text_embeddings = np.load('text_embeddings.npy')
print(f"Shape of the loaded text embeddings: {text_embeddings.shape}")

# --- Step 6: Combine All Features into a Unified Vector ---
# Now we'll merge the three processed feature sets into a single, wide vector for each user.
# We use np.hstack to stack them horizontally.
print("\nCombining all features to create the unified user vectors...")
# Normalize each feature group individually
cat_norm = normalize(categorical_df, norm='l2', axis=1)
num_norm = normalize(numerical_scaled, norm='l2', axis=1)
text_norm = text_embeddings / np.linalg.norm(text_embeddings, axis=1, keepdims=True)

# Combine and then normalize again
combined_features = np.hstack([cat_norm, num_norm, text_norm])

# Final L2 normalization
row_norms = np.linalg.norm(combined_features, axis=1, keepdims=True)
row_norms[row_norms == 0] = 1e-10
unified_user_vectors = combined_features / row_norms
print(f"Shape of the combined feature matrix: {combined_features.shape}")

print(f"Shape of the final normalized user vectors: {unified_user_vectors.shape}")

# --- Step 8: Save the Final Unified Vectors ---
# This is the final output of our feature engineering. This file contains the
# complete numerical representation of every student in your dataset.
np.save('unified_user_vectors.npy', unified_user_vectors)
print("\nUnified user vectors successfully created and saved to 'unified_user_vectors.npy'")

"""## 6. Calculate Similarity Matrix and Generate Recommendations

This section calculates the cosine similarity between all pairs of unified user vectors to create a similarity matrix. A function is then defined and used to generate friend recommendations for a given student based on these similarity scores.
"""

# --- Step 1: Import Necessary Libraries and Load Data ---
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load the original DataFrame to get student names for our recommendations.
# Make sure to use the same CSV file you started with.
df = pd.read_csv('profiles_processed.csv')

# Load the final unified user vectors that you saved in the last step.
print("Loading the unified user vectors...")
unified_user_vectors = np.load('unified_user_vectors.npy')
print(f"Successfully loaded user vectors with shape: {unified_user_vectors.shape}")


# --- Step 2: Calculate the Similarity Matrix ---
# This step remains the same.
print("\nCalculating the user-to-user similarity matrix...")
similarity_matrix = cosine_similarity(unified_user_vectors)
similarity_matrix = np.clip(similarity_matrix, -1, 1)
print(f"Similarity matrix calculated. Shape: {similarity_matrix.shape}")


# --- Step 3: Build a More Robust Recommendation Function ---

# Create a mapping from student_id to the DataFrame's integer position (index).
# This is the crucial fix to prevent mismatches.
id_to_index = pd.Series(df.index, index=df.student_id)


def get_friend_recommendations(student_id, top_n=10):
    """
    Generates friend recommendations for a given student ID using a robust index mapping.

    Args:
        student_id (int): The ID of the student for whom to generate recommendations.
        top_n (int): The number of top recommendations to return.

    Returns:
        A pandas DataFrame with the names and IDs of recommended students and their similarity scores.
    """
    # Check if the student_id exists in our mapping
    if student_id not in id_to_index:
        return f"Student with ID {student_id} not found."

    # Get the integer index (position) of the student from our mapping
    student_index = id_to_index[student_id]

    # Get the similarity scores for this student against all other students
    similarity_scores = list(enumerate(similarity_matrix[student_index]))

    # Sort the students based on their similarity scores in descending order
    sorted_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)

    # Get the indices and scores of the top N most similar students.
    # We start from index 1 to exclude the student themselves.
    top_students = sorted_scores[1:top_n+1]

    # Extract only the integer indices from the sorted list of tuples
    top_student_indices = [i[0] for i in top_students]

    # Extract the similarity scores from the sorted list of tuples
    top_student_scores = [i[1] for i in top_students]

    # Get the names and details of the top N students using their integer positions (.iloc)
    recommended_students = df.iloc[top_student_indices]

    # Create a DataFrame to display the results cleanly
    # Adding student_id to the output makes it easier to verify.
    recommendations_df = pd.DataFrame({
        'Name': recommended_students['Name'],
        'student_id': recommended_students['student_id'],
        'Similarity Score': top_student_scores
    })

    return recommendations_df

# --- Step 4: Get Recommendations for an Example Student ---
# Let's test our corrected function.
example_student_id = 15001
student_name = df[df['student_id'] == example_student_id]['Name'].values

print(f"\n--- Generating Top 10 Friend Recommendations for {student_name} (ID: {example_student_id}) ---")
recommendations = get_friend_recommendations(example_student_id, top_n=10)

# Display the recommendations
print(recommendations)

"""## 7. Visualize User Embeddings

This final section uses dimensionality reduction techniques (like t-SNE) to reduce the high-dimensional user vectors to 2D, allowing for visualization of the user embedding space and highlighting specific users.
"""

# --- Visualization of User Embeddings ---

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Load your unified vectors
unified_user_vectors = np.load('unified_user_vectors.npy')
df = pd.read_csv('profiles_processed.csv')

# ----- 1. Dimensionality Reduction ----
reducer = TSNE(n_components=2, perplexity=40, learning_rate=200, random_state=42)
emb_2d = reducer.fit_transform(unified_user_vectors)

# ----- 2. Plot -----
plt.figure(figsize=(12, 8))
plt.scatter(emb_2d[:,0], emb_2d[:,1], s=8, alpha=0.5, c='steelblue')
plt.title("User Embedding Space (t-SNE projection)")
plt.xlabel("Dimension 1"); plt.ylabel("Dimension 2")

# Highlight one or more students
def highlight(name, color='red', label=None):
    idx = df.index[df['Name'].str.contains(name, case=False, na=False)]
    if len(idx) > 0:
        plt.scatter(emb_2d[idx,0], emb_2d[idx,1], s=80, color=color, label=label or name)
        for i in idx:
            plt.text(emb_2d[i,0]+0.5, emb_2d[i,1]+0.5, df.loc[i,'Name'].split()[0], fontsize=9)

highlight("Thomas Ibarra", color='red')
highlight("Robert Burton", color='orange')
plt.legend()
plt.show()



import torch, torch_geometric
from sentence_transformers import SentenceTransformer
print(torch.__version__)
print(torch_geometric.__version__)
print("CUDA available:", torch.cuda.is_available())

# ============================================================
# SOCIAL HUB FRIEND RECOMMENDER - GRAPH NEURAL NETWORK (GNN)
# ============================================================

# ---- 0. Imports ----
import os, numpy as np, pandas as pd, torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

from torch_geometric.data import Data
from torch_geometric.utils import to_undirected, remove_self_loops
from torch_geometric.transforms import RandomLinkSplit
from torch_geometric.nn import SAGEConv

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ---- 1. Load your embeddings + student data ----
X = np.load("unified_user_vectors.npy").astype(np.float32)
df = pd.read_csv("profiles_processed.csv")
assert len(df) == X.shape[0], "Mismatch between df rows and embeddings!"

num_nodes, feat_dim = X.shape
print(f"Nodes: {num_nodes}, Feature dim: {feat_dim}")

# ---- 2. Build sparse k-NN graph (synthetic friendships) ----
k = 8  # neighbors per node
nbrs = NearestNeighbors(n_neighbors=k+1, metric="cosine")
nbrs.fit(X)
dist, idx = nbrs.kneighbors(X)

edge_src, edge_dst = [], []
for i in range(num_nodes):
    for j in idx[i][1:]:
        edge_src.append(i)
        edge_dst.append(int(j))
edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)
edge_index, _ = remove_self_loops(edge_index)
edge_index = to_undirected(edge_index)
print("Edges:", edge_index.shape[1])

# ---- 3. Wrap into a PyG Data object + train/val/test split ----
x = torch.from_numpy(X)
data = Data(x=x, edge_index=edge_index)
splitter = RandomLinkSplit(num_val=0.1, num_test=0.1, is_undirected=True,
                           add_negative_train_samples=True)
train_data, val_data, test_data = splitter(data)
print("Split done.")

# ---- 4. Define GraphSAGE model ----
class GraphSAGE(nn.Module):
    def __init__(self, in_dim, hidden=256, out_dim=128):
        super().__init__()
        self.conv1 = SAGEConv(in_dim, hidden)
        self.conv2 = SAGEConv(hidden, out_dim)
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return F.normalize(x, p=2, dim=-1)

def edge_score(z, edge_label_index):
    src, dst = edge_label_index
    return (z[src] * z[dst]).sum(dim=-1)

# ---- 5. Training setup ----
model = GraphSAGE(in_dim=feat_dim).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-4)

def run_epoch(part, optimizer=None):
    model.train(mode=optimizer is not None)
    x = part.x.to(device)
    z = model(x, train_data.edge_index.to(device))  # always use training graph
    logits = edge_score(z, part.edge_label_index.to(device))
    labels = part.edge_label.to(device).float()
    loss = F.binary_cross_entropy_with_logits(logits, labels)

    with torch.no_grad():
        probs = torch.sigmoid(logits).cpu().numpy()
        y_true = labels.cpu().numpy()
        auc = roc_auc_score(y_true, probs)
        ap  = average_precision_score(y_true, probs)

    if optimizer is not None:
        optimizer.zero_grad(); loss.backward(); optimizer.step()
    return float(loss), float(auc), float(ap)

# ---- 6. Train model ----
best_val_auc, best_state = -1, None
for epoch in range(1, 51):
    tr_loss, tr_auc, tr_ap = run_epoch(train_data, optimizer)
    with torch.no_grad():
        vl_loss, vl_auc, vl_ap = run_epoch(val_data)
    if vl_auc > best_val_auc:
        best_val_auc = vl_auc
        best_state = {k:v.cpu() for k,v in model.state_dict().items()}
    if epoch % 5 == 0:
        print(f"Epoch {epoch:02d} | "
              f"train loss {tr_loss:.4f}, AUC {tr_auc:.3f} | "
              f"val loss {vl_loss:.4f}, AUC {vl_auc:.3f}")
model.load_state_dict({k:v.to(device) for k,v in best_state.items()})
print("Best val AUC:", best_val_auc)

# ---- 7. Test evaluation ----
with torch.no_grad():
    te_loss, te_auc, te_ap = run_epoch(test_data)
print(f"TEST | loss {te_loss:.4f}, AUC {te_auc:.3f}, AP {te_ap:.3f}")

# ---- 8. Friend recommendations ----
@torch.no_grad()
def recommend_friends(node_id: int, top_k: int = 10):
    model.eval()
    z = model(data.x.to(device), train_data.edge_index.to(device)).cpu()
    sims = z @ z[node_id]
    sims[node_id] = -1e9
    # remove existing neighbors
    src, dst = train_data.edge_index
    neigh = set(dst[src == node_id].tolist()) | set(src[dst == node_id].tolist())
    sims[list(neigh)] = -1e9
    top_idx = torch.topk(sims, k=top_k).indices.numpy()
    top_scores = sims[top_idx].numpy()
    return top_idx, top_scores

# Example: student at row 0 (Thomas Ibarra)
idx = 0
rec_idx, rec_scores = recommend_friends(idx, top_k=10)
out = df.iloc[rec_idx][["Name","Major","Year","Hobbies","GPA"]].copy()
out["score"] = np.round(rec_scores,4)
print("\nTop recommendations for:", df.iloc[idx]["Name"])
print(out)

# ---- 9. Optional t-SNE visualization ----
with torch.no_grad():
    z = model(data.x.to(device), train_data.edge_index.to(device)).cpu().numpy()
tsne = TSNE(n_components=2, perplexity=40, learning_rate=200, random_state=42)
z2 = tsne.fit_transform(z)
plt.figure(figsize=(10,8))
plt.scatter(z2[:,0], z2[:,1], s=6, alpha=0.5)
plt.title("GraphSAGE Embedding Space (t-SNE)")
plt.xlabel("dim-1"); plt.ylabel("dim-2")
plt.show()